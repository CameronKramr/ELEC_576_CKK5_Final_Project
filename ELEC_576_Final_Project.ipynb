{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ccb6e83-40ee-45fd-902d-2becdad67ce6",
   "metadata": {},
   "source": [
    "# Main Function Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6ea43d-aacf-4a84-bbf6-a8fc3ec8237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TORCH_SHOW_CPP_STACKTRACES = True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "from torch import _logging as logging\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "af3927d9-c215-4d8b-844b-61df025b8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_str = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "logging_dir = None\n",
    "if logging_dir is None:\n",
    "    runs_dir = Path(\"./\") / Path(f\"rnn_runs/\")\n",
    "    runs_dir.mkdir(exist_ok = True)\n",
    "\n",
    "    logging_dir = runs_dir / Path(f\"{datetime_str}\")\n",
    "\n",
    "    logging_dir.mkdir(exist_ok = True)\n",
    "    logging_dir = str(logging_dir.absolute())\n",
    "\n",
    "writer = SummaryWriter(log_dir=logging_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39e152-ca56-4713-bdea-e3ccabaa6eba",
   "metadata": {},
   "source": [
    "# MLBF For Realzies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6ceb049c-51cd-4454-bdae-94fb3e530fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Outer loop: 0\n",
      "Plain V\n",
      "Plain V\n",
      "Completed Outer loop: 0\n",
      "Begin Outer loop: 1\n",
      "Plain V\n",
      "Plain V\n",
      "Completed Outer loop: 1\n",
      "Begin Outer loop: 2\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 2\n",
      "Begin Outer loop: 3\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 3\n",
      "Begin Outer loop: 4\n",
      "Restricted V\n",
      "Restricted V\n",
      "Backpropagating Loss 4\n",
      "Completed Outer loop: 4\n",
      "Begin Outer loop: 5\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 5\n",
      "Begin Outer loop: 6\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 6\n",
      "Begin Outer loop: 7\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 7\n",
      "Begin Outer loop: 8\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 8\n",
      "Begin Outer loop: 9\n",
      "Restricted V\n",
      "Restricted V\n",
      "Backpropagating Loss 9\n",
      "Completed Outer loop: 9\n",
      "Begin Outer loop: 10\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 10\n",
      "Begin Outer loop: 11\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 11\n",
      "Begin Outer loop: 12\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 12\n",
      "Begin Outer loop: 13\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 13\n",
      "Begin Outer loop: 14\n",
      "Restricted V\n",
      "Restricted V\n",
      "Backpropagating Loss 14\n",
      "Completed Outer loop: 14\n",
      "Begin Outer loop: 15\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 15\n",
      "Begin Outer loop: 16\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 16\n",
      "Begin Outer loop: 17\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 17\n",
      "Begin Outer loop: 18\n",
      "Restricted V\n",
      "Restricted V\n",
      "Completed Outer loop: 18\n",
      "Begin Outer loop: 19\n",
      "Restricted V\n",
      "Restricted V\n",
      "Backpropagating Loss 19\n",
      "Completed Outer loop: 19\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "#torch._logging.set_logs(dynamo = logging.INFO)\n",
    "\n",
    "class MLBF(nn.Module):\n",
    "    def __init__(self,\n",
    "                users,\n",
    "                antennas,\n",
    "                depth=1,\n",
    "                hidden_layers=200,\n",
    "                outer_iterations=500,\n",
    "                inner_iterations=10,\n",
    "                update_rate=5 \n",
    "                ):\n",
    "        super(MLBF, self).__init__()\n",
    "\n",
    "        # Performance Variables\n",
    "        self.M = antennas\n",
    "        self.N = users\n",
    "        self.sigma = 1e-6\n",
    "        self.mu = 1e-6\n",
    "        self.P = 10\n",
    "        self.w_ts = 1\n",
    "\n",
    "        # Iteration related variables\n",
    "        self.K = inner_iterations #Max inner loops for V\n",
    "        self.I = inner_iterations #Max inner loops for u\n",
    "        self.J = inner_iterations #Max inner loops for w\n",
    "        self.T = outer_iterations #Max outer loops for each sub-problem\n",
    "        self.tup = update_rate    #Update interval for LSTMs outer loop\n",
    "        self.S = int(self.T / self.tup) #Maximum number of updates\n",
    "\n",
    "        # Problem/solution tensors\n",
    "        # H is the channel matrix. This will be randomly generated\n",
    "        self.H = torch.randn((users, antennas), dtype=torch.complex64) / (users*antennas)\n",
    "    \n",
    "        # V is beamsteering matrix. \n",
    "            # Each row a user. Each column a transmit antenna\n",
    "        self.V = torch.randn((users, antennas), dtype=torch.complex64)\n",
    "        self.V = self.V / torch.linalg.norm(self.V) * torch.sqrt(torch.tensor(self.P))/100\n",
    "    \n",
    "        # Optimized/Learned importance for each user\n",
    "        self.w = torch.randn((users, 1), dtype=torch.complex64)/users\n",
    "    \n",
    "        # Receiver gain for each user\n",
    "        self.u = torch.randn((users, 1), dtype=torch.complex64)/users\n",
    "        \n",
    "        # System design user priorities\n",
    "        self.Alpha = torch.ones((users, 1), dtype=torch.complex64)/users\n",
    "        \n",
    "        # System design user priorities\n",
    "        self.e = torch.zeros((users, 1), dtype=torch.complex64)\n",
    "\n",
    "        # Neural Networks for solving optimization problem\n",
    "        # Network for updating the optimized user weights\n",
    "        self.w_net = nn.LSTM(input_size = users, \n",
    "                                           hidden_size = users, \n",
    "                                           num_layers=hidden_layers, \n",
    "                                           bias=True, \n",
    "                                           batch_first=False, \n",
    "                                           dropout=0.0, \n",
    "                                           bidirectional=False, \n",
    "                                           proj_size=0, \n",
    "                                           device=None, \n",
    "                                           dtype=torch.complex64)\n",
    "        #self.w_norm = F.normalize(input = users)\n",
    "        # Network for updating the receiver gains\n",
    "        self.u_net = nn.LSTM(input_size = users, \n",
    "                                           hidden_size = users, \n",
    "                                           num_layers=hidden_layers, \n",
    "                                           bias=True, \n",
    "                                           batch_first=False, \n",
    "                                           dropout=0.0, \n",
    "                                           bidirectional=False, \n",
    "                                           proj_size=0, \n",
    "                                           device=None, \n",
    "                                           dtype=torch.complex64)\n",
    "        #self.u_norm = F.normalize(input = users)\n",
    "        # Network for updating the V beamforming matrix\n",
    "        self.V_net = nn.LSTM(input_size = users * antennas, # Input is one parameter for each user to receive from an antenna\n",
    "                                           hidden_size = users * antennas, #users * antennas, \n",
    "                                           num_layers=hidden_layers,\n",
    "                                           bias=True, \n",
    "                                           batch_first=False, \n",
    "                                           dropout=0.0, \n",
    "                                           bidirectional=False, \n",
    "                                           proj_size=0, \n",
    "                                           device=None, \n",
    "                                           dtype=torch.complex64)\n",
    "        #self.V_norm = F.normalize(input = users * antennas)\n",
    "        #Configure optimizer for this network\n",
    "        \n",
    "        self.optim = optim.Adam(self.parameters(), lr = 1e-4)\n",
    "        \n",
    "    #----------------------------- Calculate MSE -----------------------------\n",
    "    def calc_MSE(self):\n",
    "        # Calculate MSE (e vector) according to (5) in [5]\n",
    "        for w, u, v, h, i in zip(self.w, \n",
    "                                 self.u, \n",
    "                                 self.V.t(), \n",
    "                                 self.H.t(), \n",
    "                                 range(self.N)):\n",
    "            # Error contribution of how well beam points at current user + noise\n",
    "            self.e[i] = torch.pow(u*torch.sqrt(torch.vdot(h.t(), v)) - 1, 2) + u*self.sigma\n",
    "\n",
    "            # Error contribution of pointing at other users\n",
    "            # Equation 5 from [5]\n",
    "            for v_j, j in zip(self.V.t(),\n",
    "                              range(self.N)):\n",
    "                if(j != i):\n",
    "                    self.e[i] = self.e[i] + (torch.abs(u)**2)*torch.vdot(h.t(),v_j)\n",
    "    #----------------------------- Calculate Global Loss -----------------------------        \n",
    "    def global_loss(self):\n",
    "        # Calculate sum\n",
    "        loss = torch.tensor(0.0)\n",
    "        #Sum operation for (9) in [5]\n",
    "        for w, alpha, e, i in zip(\n",
    "                                 self.w, \n",
    "                                 self.Alpha, \n",
    "                                 self.e, \n",
    "                                 range(self.N)\n",
    "                                ):\n",
    "            loss = loss + alpha*( w*e - torch.log2(w))\n",
    "            \n",
    "        #Final Loss operation for (9) in [5]\n",
    "        return loss + self.mu*torch.trace(self.V@(self.V.t().conj())) - self.mu*self.P\n",
    "        \n",
    "    #----------------------------- Calculate full sum -----------------------------\n",
    "    def calc_full_sum(self):\n",
    "\n",
    "        #Calculate the sum vector\n",
    "        sum_total = torch.zeros(self.N, dtype=torch.complex64)\n",
    "        \n",
    "        #Outer Loop\n",
    "        for h, i in zip( self.H.t(), \n",
    "                         range(self.N)):\n",
    "            #Inner Loop\n",
    "            for v, j in zip(self.V.t(),\n",
    "                            range(self.N)\n",
    "                            ):\n",
    "                \n",
    "                #Track the sum of every squared product\n",
    "                sum_total[i] = sum_total[i] + torch.vdot(h, v)\n",
    "        return sum_total\n",
    "\n",
    "    #----------------------------- Calculate du -----------------------------\n",
    "    def calc_u_grad(self):\n",
    "        u_f = self.u.detach().clone()\n",
    "        #u_f.requires_grad_()\n",
    "        #print(u_f.shape)\n",
    "        sum_total = self.calc_full_sum()\n",
    "\n",
    "        for v, h, i in zip(\n",
    "                        self.V.t(),\n",
    "                        self.H.t(), \n",
    "                        range(self.N)):\n",
    "        \n",
    "            # using EQ 7. from [5]\n",
    "            u_f[i,0] = torch.vdot(h, v) / (sum_total[i] + self.sigma)\n",
    "            \n",
    "        #return u_f.backward(retain_graph=True).grad\n",
    "        return F.normalize(u_f - self.u)\n",
    "\n",
    "    #----------------------------- Calculate dw -----------------------------\n",
    "    def calc_w_grad(self):\n",
    "        #using EQ 6. from [5]\n",
    "        \n",
    "        w_f = torch.zeros((self.N,1), dtype=torch.complex64)\n",
    "        sum_total = self.calc_full_sum()\n",
    "\n",
    "        for v, h, i in zip(\n",
    "                        self.V.t(),\n",
    "                        self.H.t(), \n",
    "                        range(self.N)\n",
    "                        ):\n",
    "            # using EQ 6. from [5]\n",
    "            w_f[i] =  (sum_total[i] + self.sigma) / (sum_total[i] - torch.vdot(h, v) + self.sigma)\n",
    "        \n",
    "        return F.normalize(w_f - self.w)\n",
    "\n",
    "    #----------------------------- Calculate dV -----------------------------\n",
    "    def calc_V_grad(self):\n",
    "        grad_V = torch.zeros((self.N , self.M), dtype=torch.complex64)\n",
    "        \n",
    "        A = torch.zeros((self.N, self.N), dtype=torch.complex64)\n",
    "\n",
    "        #This is the PGD cost function from [12] to circumvent the matrix inversion\n",
    "        #Calculate A using EQ 8. from [12]\n",
    "        for w, u, h, alpha, i in zip(\n",
    "                                     self.w, \n",
    "                                     self.u,\n",
    "                                     self.H.t(),\n",
    "                                     self.Alpha,\n",
    "                                     range(self.N)\n",
    "                                    ):\n",
    "            A = A + alpha*w*(torch.abs(u)**2)*h.t().conj()@h\n",
    "        # Cost function for V according to [12] \n",
    "        for v, w, u, h, alpha, i in zip(\n",
    "                                     self.V.t(),\n",
    "                                     self.w, \n",
    "                                     self.u,\n",
    "                                     self.H.t(),\n",
    "                                     self.Alpha,\n",
    "                                     range(self.N)\n",
    "                                    ):\n",
    "            grad_V[:,i] = 2*A@v - 2*alpha*w*u*h\n",
    "        #print(grad_V)wi\n",
    "        return F.normalize(grad_V, dim=0)\n",
    "            \n",
    "    #Cap V to a maximum\n",
    "    def Restrict_V(self, V):\n",
    "        if(torch.abs(torch.trace(V@V.conj().t())) <= self.P):\n",
    "            print(\"Plain V\")\n",
    "            return V\n",
    "        else:\n",
    "            print(\"Restricted V\")\n",
    "            return V / torch.linalg.norm(V) * torch.sqrt(torch.tensor(self.P))\n",
    "        \n",
    "    def run(self, Channel):\n",
    "        # PyTorch order of operations:\n",
    "        # [X] zero grad\n",
    "        # [X] run model\n",
    "            # [X] calc grad_f(u)\n",
    "            # [X] run u_net to update u\n",
    "            # [X] calc grad_f(w)\n",
    "            # [X] run w_net to update w        will use LSTM with projections of corresponding size. Default: 0\n",
    "            # [X] calc grad_f(V)\n",
    "            # [X] run V_net to update V\n",
    "            # [X] Restrict V to P\n",
    "        # [X] loss = calculate criterion\n",
    "        # [X] loss.backwards\n",
    "        # [X] optimizer.step\n",
    "\n",
    "        # zero grad\n",
    "        #Initialize LSTM states\n",
    "        u_C = None\n",
    "        w_C = None\n",
    "        V_C = None\n",
    "\n",
    "        #Create the loss variable\n",
    "        loss = torch.tensor(0.0)\n",
    "        self.optim.zero_grad()\n",
    "        for t in range(self.T):\n",
    "            print(f\"Begin Outer loop: {t}\")\n",
    "            \n",
    "            # Run u_net\n",
    "            for i in range(self.I):\n",
    "                #Compute gradient of f(u)        will use LSTM with projections of corresponding size. Default: 0\n",
    "                u_grad = self.calc_u_grad()\n",
    "                writer.add_scalar('u_grad / Loops', torch.linalg.norm(u_grad).item(), t*self.tup + i)\n",
    "                u_H, u_C = self.u_net(u_grad.t(), u_C)\n",
    "                #print(\"u: \" + str(self.u.shape))\n",
    "                #print(\"uH: \" + str(u_H.shape))\n",
    "                writer.add_scalar('u_H / Loops', torch.linalg.norm(u_H).item(), t*self.tup + i)\n",
    "                self.u = self.u + u_H.t()\n",
    "                writer.add_scalars('abs(u_user)/Loops', \n",
    "                                   {f\"u_{l}\":u.item() for l, u in enumerate(self.u.abs())}, \n",
    "                                   t*self.tup + i)\n",
    "                writer.add_scalars('re(u_H)/Loops', \n",
    "                                   {f\"re(u)_{l}\":u.item() for l, u in enumerate(u_H.real.t())}, \n",
    "                                   t*self.tup + i)\n",
    "                writer.add_scalars('im(u_H)/Loops', \n",
    "                                   {f\"im(u)_{l}\":u.item() for l, u in enumerate(u_H.imag.t())}, \n",
    "                                   t*self.tup + i)\n",
    "                \n",
    "            # Run w_net\n",
    "            for j in range(self.J):\n",
    "                #Compute gradient of f(w)\n",
    "                w_grad = self.calc_w_grad()\n",
    "                writer.add_scalar('w_grad / Loops', torch.linalg.norm(w_grad).item(), t*self.tup + j)\n",
    "                w_H, w_C = self.w_net(w_grad.t(), w_C)\n",
    "                writer.add_scalar('w_H / Loops', torch.linalg.norm(w_H).item(), t*self.tup + j)\n",
    "                #print(\"wH: \" + str(w_H.shape))\n",
    "                self.w = self.w + w_H.t()\n",
    "                writer.add_scalars('abs(w_user)/Loops', \n",
    "                                   {f\"w_{l}\":w.item() for l, w in enumerate(self.w.abs())}, \n",
    "                                   t*self.tup + j)\n",
    "                #print(w_H.shape)\n",
    "                writer.add_scalars('re(w_H)/Loops', \n",
    "                                   {f\"re(w)_{l}\":w.item() for l, w in enumerate(w_H.real.t())}, \n",
    "                                   t*self.tup + j)\n",
    "                writer.add_scalars('im(w_H)/Loops', \n",
    "                                   {f\"im(w)_{l}\":w.item() for l, w in enumerate(w_H.imag.t())}, \n",
    "                                   t*self.tup + j)\n",
    "            # Run V_net\n",
    "            for k in range(self.K):\n",
    "                #Compute gradient of f(V)\n",
    "                V_grad = self.calc_V_grad().reshape(self.N*self.M,1)\n",
    "                writer.add_scalar('V_grad / Loops', torch.linalg.norm(V_grad).item(), t*self.tup + k)\n",
    "\n",
    "                V_H, V_C = self.V_net(V_grad.t(), V_C)\n",
    "                writer.add_scalar('V_H / Loops', torch.linalg.norm(V_H).item(), t*self.tup + k)\n",
    "                \n",
    "                dV = torch.reshape(V_H, (self.N, self.M))\n",
    "                self.V = self.V + dV\n",
    "                writer.add_scalar('dV_ant / Loops', torch.linalg.norm(dV).item(), t*self.tup + k)\n",
    "                writer.add_scalars('mag(dV_antenna)/Loops', \n",
    "                                   {f\"dv_{l}\":torch.linalg.norm(v).item() for l, v in enumerate(dV.t().abs())}, \n",
    "                                   t*self.tup + k)\n",
    "                \n",
    "                self.V = self.Restrict_V(self.V)\n",
    "                writer.add_scalars('mag(V_antenna)/Loops', \n",
    "                                   {f\"v_{l}\":torch.linalg.norm(v).item() for l, v in enumerate(self.V.t().abs())}, \n",
    "                                   t*self.tup + k)\n",
    "\n",
    "            #Sum the loss weighted by w_ts\n",
    "            self.calc_MSE()\n",
    "            loss = loss.detach().clone() + self.w_ts*torch.abs(self.global_loss())/self.tup\n",
    "            if(torch.isnan(loss)):\n",
    "                print(\"Loss Diverged!\")\n",
    "                break\n",
    "            \n",
    "            # it's time to udpate the networks to the accumulated loss function\n",
    "            if(t % self.tup == self.tup - 1):\n",
    "                print(f\"Backpropagating Loss {t}\")\n",
    "                # Backwards\n",
    "                writer.add_scalar('Global Loss / Outer Loops', loss.item(), t)\n",
    "                loss.backward(retain_graph=False)\n",
    "\n",
    "                # Optimizer step\n",
    "                self.optim.step()\n",
    "                \n",
    "                #Break-off old versions of V, w, e, and u to prepare for next pass\n",
    "                self.V.detach_()\n",
    "                self.w.detach_()\n",
    "                self.u.detach_()\n",
    "                self.e.detach_()\n",
    "\n",
    "                #Reset hidden state variables for new batch of optimization\n",
    "                V_C = None\n",
    "                w_C = None\n",
    "                u_C = None\n",
    "\n",
    "                #Zero out the error\n",
    "                self.e.zero_()\n",
    "                loss.zero_()\n",
    "                \n",
    "                # zero grad --After we've updated the gradient we need to clear it to accumulate again\n",
    "                self.optim.zero_grad()\n",
    "                \n",
    "            print(f\"Completed Outer loop: {t}\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "guy = MLBF( users = 2,\n",
    "            antennas = 2,\n",
    "            depth=1,\n",
    "            hidden_layers=200,\n",
    "            outer_iterations=20,\n",
    "            inner_iterations=2,\n",
    "            update_rate=5)\n",
    "\n",
    "#Make a channel\n",
    "channel = compute_channel(2, 2)\n",
    "\n",
    "guy.H = channel\n",
    "\n",
    "guy.run(channel)\n",
    "\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9bd0b6-21ca-42c4-86eb-df292c315449",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Misunderstood MLBF Class function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7fdfabe5-a175-4d43-9752-fd6f310a85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Other_Strangeness(nn.Module):\n",
    "    def __init__(self, \n",
    "                 users, \n",
    "                 antennas, \n",
    "                 alpha = None):\n",
    "        super(Other_Strangeness, self).__init__()\n",
    "        \n",
    "        # Basic features to track\n",
    "        self.Antenna = antennas\n",
    "        self.N = users\n",
    "\n",
    "        # Meta Parameters:\n",
    "\n",
    "        # Optimizers to be used by each RNN for V, u, and w\n",
    "\n",
    "        # Network for updating the optimized user weights\n",
    "        self.w_net = nn.Sequential(nn.LSTM(input_size = users, \n",
    "                                           hidden_size = 1, \n",
    "                                           num_layers=1, \n",
    "                                           bias=True, \n",
    "                                           batch_first=False, \n",
    "                                           dropout=0.0, \n",
    "                                           bidirectional=False, \n",
    "                                           proj_size=0, \n",
    "                                           device=None, \n",
    "                                           dtype=torch.complex64)\n",
    "                                  )\n",
    "\n",
    "        # Network for updating the receiver gains\n",
    "        self.u_net = nn.Sequential(nn.LSTM(input_size = users, \n",
    "                                           hidden_size = 1, \n",
    "                                           num_layers=1, \n",
    "                                           bias=True, \n",
    "                                           batch_first=False, \n",
    "                                           dropout=0.0, \n",
    "                                           bidirectional=False, \n",
    "                                           proj_size=0, \n",
    "                                           device=None, \n",
    "                                           dtype=torch.complex64)\n",
    "                                  )\n",
    "        \n",
    "        # Network for updating the V beamforming matrix\n",
    "        self.V_net = nn.Sequential(nn.LSTM(input_size = users * antennas, # Input is one parameter for each user to receive from an antenna\n",
    "                                           hidden_size = 1, \n",
    "                                           num_layers=1, \n",
    "                                           bias=True, \n",
    "                                           batch_first=False, \n",
    "                                           dropout=0.0, \n",
    "                                           bidirectional=False, \n",
    "                                           proj_size=0, \n",
    "                                           device=None, \n",
    "                                           dtype=torch.complex64)\n",
    "                                  )\n",
    "        \n",
    "        # H is the channel matrix. This will be randomly generated\n",
    "        self.H = torch.ones((users, antennas), dtype=torch.complex64) / (users*antennas)\n",
    "\n",
    "        # V is beamsteering matrix. \n",
    "            # Each row a user. Each column a transmit antenna\n",
    "        self.V = torch.ones((users, antennas), dtype=torch.complex64)\n",
    "\n",
    "        # Optimized/Learned importance for each user\n",
    "        self.w = torch.ones((users, 1), dtype=torch.complex64)\n",
    "\n",
    "        # Receiver gain for each user\n",
    "        self.u = torch.ones((users, 1), dtype=torch.complex64)\n",
    "\n",
    "        # System designer can assign an importance to each user\n",
    "        if alpha is None:\n",
    "            self.Alpha = torch.ones((users,1), dtype=torch.complex64)/users\n",
    "        else:\n",
    "            self.Alpha = alpha\n",
    "\n",
    "        # Mean-Square Error for each user:\n",
    "        self.e = torch.ones((users, 1), dtype=torch.complex64)\n",
    "\n",
    "        #This is actually sigma^2\n",
    "        self.sigma = 0.1\n",
    "\n",
    "        self.mu = 1\n",
    "    \n",
    "    def calc_MSE(self):\n",
    "        for w, u, v, h, i in zip(self.w, \n",
    "                                 self.u, \n",
    "                                 self.V.t(), \n",
    "                                 self.H.t(), \n",
    "                                 range(self.N)):\n",
    "            # Error contribution of how well beam points at current user + noise\n",
    "            self.e[i] = torch.pow(u*torch.conj(h.transpose(-1,0))@v - 1,2) + u*self.sigma\n",
    "\n",
    "            # Error contribution of pointing at other users\n",
    "            for j in range(self.N):\n",
    "                if(j != i):\n",
    "                    self.e[i] = self.e[i] + torch.pow(u*torch.conj(h.transpose(-1,0))@v,2)\n",
    "    \n",
    "    def calc_update_w_u(self):\n",
    "        #Outer Loop\n",
    "        for h, i in zip( self.H.t(), \n",
    "                         range(self.N)):\n",
    "            sum_total = 0\n",
    "            sum_exclusive = 0\n",
    "            \n",
    "            #Inner Loop\n",
    "            for v, j in zip(self.V.t(),\n",
    "                            range(self.N)\n",
    "                            ):\n",
    "\n",
    "                #every loop, a product is used\n",
    "                prod = torch.vdot(h, v)\n",
    "                \n",
    "                #Track the sum of every squared product \n",
    "                sum_total = sum_total + prod**2\n",
    "\n",
    "                #Exclusive sum for where i != j\n",
    "                if j != i:\n",
    "                    sum_exclusive = sum_exclusive + prod**2\n",
    "\n",
    "                #Set the numerator of u\n",
    "                if j == i:\n",
    "                    self.u[i] = prod\n",
    "\n",
    "            #Update u_i, w_i\n",
    "            # using EQ 7. from [5]\n",
    "            self.u[i] = self.u[i] / (sum_total + self.sigma)\n",
    "            #using EQ 6. from [5]\n",
    "            self.w[i] = (sum_total + self.sigma) / (sum_exclusive + self.sigma)\n",
    "                \n",
    "    def calc_update_v(self):\n",
    "        A = 0#torch.zeros((self.N, self.N), dtype=self.H.dtype)\n",
    "        #Calculate (A + uI)^-1 using EQ 8. from [5]\n",
    "        for w, u, h, alpha, i in zip(\n",
    "                                     self.w, \n",
    "                                     self.u,\n",
    "                                     self.H.t(),\n",
    "                                     self.Alpha,\n",
    "                                     range(self.N)\n",
    "                                    ):           \n",
    "            A = A + alpha*w*(torch.abs(u)**2)*h.t().conj()@h\n",
    "        A_prod = torch.linalg.inv(A + self.mu*torch.eye(self.N))\n",
    "\n",
    "        #Calculate v_i\n",
    "        for w, u, h, alpha, i in zip(\n",
    "                                     self.w, \n",
    "                                     self.u,\n",
    "                                     self.H.t(),\n",
    "                                     self.Alpha,\n",
    "                                     range(self.N)\n",
    "                                    ):\n",
    "            self.V[:,i] = alpha * u * w * (h @ A_prod)\n",
    "\n",
    "    # Update w, u, V\n",
    "    def calc_update(self):\n",
    "        pass\n",
    "        \n",
    "model = Other_Strangeness(4,7)\n",
    "model.calc_MSE()\n",
    "model.calc_update_w_u()\n",
    "model.calc_update_v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5d680c8-4bde-4c97-9b08-8b74646479f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5598,  1.6137,  0.9035, -0.3313, -0.6225,  0.8683,  1.5788],\n",
       "        [-0.2687, -0.7519, -0.1923, -0.2689,  0.6816,  1.2936, -1.0451],\n",
       "        [ 0.1007,  0.1129,  0.9491, -0.9735, -0.2605, -1.6384, -0.5019],\n",
       "        [ 1.2305, -0.4356,  0.4193,  1.9965, -0.3644, -2.4067, -0.6000]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,7)\n",
    "y = torch.ones((4,4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b22d1d46-fa92-4ddc-ad3d-5c21ee5a4ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5598, -0.2687,  0.1007,  1.2305],\n",
       "        [ 1.6137, -0.7519,  0.1129, -0.4356],\n",
       "        [ 0.9035, -0.1923,  0.9491,  0.4193],\n",
       "        [-0.3313, -0.2689, -0.9735,  1.9965],\n",
       "        [-0.6225,  0.6816, -0.2605, -0.3644],\n",
       "        [ 0.8683,  1.2936, -1.6384, -2.4067],\n",
       "        [ 1.5788, -1.0451, -0.5019, -0.6000]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea787f99-4729-41f7-87a5-336886b1c856",
   "metadata": {},
   "source": [
    "# Compute the Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d727ab-4de4-402d-8ccf-6a6cd7fa8787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_channel(users, \n",
    "                    antennas):\n",
    "    \n",
    "    #Create the complex channel realization\n",
    "    #channel_mat = (np.random.rand(users, antennas) + 1j*np.random.rand(users, antennas))/np.sqrt(2)\n",
    "    channel_mat = torch.rand((users, antennas), dtype=torch.complex64)\n",
    "    \n",
    "    return channel_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a9ddb-6106-439c-b8cb-96e3e466c39d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Compute the WMMSE Unfolded Algorithm Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e708e7fd-5802-4876-a0d3-10215d0a9f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for WMMSE Algorithm\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdfc03-9a33-40a6-9c38-fa13ef02722f",
   "metadata": {},
   "source": [
    "Utility functions for running the main WMMSE function from \\[5\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54daf53b-bf7b-4550-943c-e5158c48ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_sum_rate(user_weights, channel, precoder, noise_power, selected_users):\n",
    "   result = 0\n",
    "   nr_of_users = np.size(channel,0)\n",
    "   \n",
    "   for user_index in range(nr_of_users):\n",
    "     if user_index in selected_users:\n",
    "       user_sinr = compute_sinr(channel, precoder, noise_power, user_index, selected_users)\n",
    "       result = result + user_weights[user_index]*np.log2(1 + user_sinr)\n",
    "    \n",
    "   return result\n",
    "\n",
    "def compute_P(Phi_diag_elements, Sigma_diag_elements, mu):\n",
    "  nr_of_BS_antennas = Phi_diag_elements.size\n",
    "  mu_array = mu*np.ones(Phi_diag_elements.size)\n",
    "  result = np.divide(Phi_diag_elements,(Sigma_diag_elements + mu_array)**2)\n",
    "  result = np.sum(result)\n",
    "  return result\n",
    "\n",
    "\n",
    "def compute_norm_of_complex_array(x):\n",
    "  result = np.sqrt(np.sum((np.absolute(x))**2))\n",
    "  return result\n",
    "\n",
    "\n",
    "def compute_sinr(channel, precoder, noise_power, user_id, selected_users):\n",
    "    nr_of_users = np.size(channel,0)\n",
    "    numerator = (np.absolute(np.matmul(np.conj(channel[user_id,:]),precoder[user_id,:])))**2\n",
    "\n",
    "    inter_user_interference = 0\n",
    "    for user_index in range(nr_of_users):\n",
    "      if user_index != user_id and user_index in selected_users:\n",
    "        inter_user_interference = inter_user_interference + (np.absolute(np.matmul(np.conj(channel[user_id,:]),precoder[user_index,:])))**2\n",
    "    denominator = noise_power + inter_user_interference\n",
    "\n",
    "    result = numerator/denominator\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_user_weights(nr_of_users, selected_users):\n",
    "  result = np.ones(nr_of_users)\n",
    "  for user_index in range(nr_of_users):\n",
    "    if not (user_index in selected_users):\n",
    "      result[user_index] = 0\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f3f3b-0803-4aac-846e-6089c2cd19f2",
   "metadata": {},
   "source": [
    "Function for iteratively solving the WMMSE optimization problem according to \\[5\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c75dba-c6c2-4818-af91-a8b57871af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_WMMSE(epsilon, channel, selected_users, total_power, noise_power, user_weights, max_nr_of_iterations, log = False):\n",
    "\n",
    "  nr_of_users = np.size(channel,0)\n",
    "  nr_of_BS_antennas = np.size(channel,1)\n",
    "  WSR=[] # to check if the WSR (our cost function) increases at each iteration of the WMMSE\n",
    "  break_condition = epsilon + 1 # break condition to stop the WMMSE iterations and exit the while\n",
    "  receiver_precoder = np.zeros(nr_of_users) + 1j*np.zeros(nr_of_users) # receiver_precoder is \"u\" in the paper of Shi et al. (it's a an array of complex scalars)\n",
    "  mse_weights = np.ones(nr_of_users) # mse_weights is \"w\" in the paper of Shi et al. (it's a an array of real scalars)\n",
    "  transmitter_precoder = np.zeros((nr_of_users, nr_of_BS_antennas)) + 1j*np.zeros((nr_of_users, nr_of_BS_antennas))# transmitter_precoder is \"v\" in the paper of Shi et al. (it's a complex matrix)\n",
    "  \n",
    "  new_receiver_precoder = np.zeros(nr_of_users) + 1j*np.zeros(nr_of_users) # for the first iteration \n",
    "  new_mse_weights = np.zeros(nr_of_users) # for the first iteration\n",
    "  new_transmitter_precoder = np.zeros((nr_of_users, nr_of_BS_antennas)) + 1j*np.zeros((nr_of_users, nr_of_BS_antennas)) # for the first iteration\n",
    "\n",
    "  \n",
    "  # Initialization of transmitter precoder\n",
    "  for user_index in range(nr_of_users):\n",
    "    if user_index in selected_users:\n",
    "      transmitter_precoder[user_index,:] = channel[user_index,:]\n",
    "  transmitter_precoder = transmitter_precoder/np.linalg.norm(transmitter_precoder)*np.sqrt(total_power)\n",
    "  \n",
    "  # Store the WSR obtained with the initialized trasmitter precoder    \n",
    "  WSR.append(compute_weighted_sum_rate(user_weights, channel, transmitter_precoder, noise_power, selected_users))\n",
    "\n",
    "  # Compute the initial power of the transmitter precoder\n",
    "  initial_power = 0\n",
    "  for user_index in range(nr_of_users):\n",
    "    if user_index in selected_users:\n",
    "      initial_power = initial_power + (compute_norm_of_complex_array(transmitter_precoder[user_index,:]))**2 \n",
    "  if log == True:\n",
    "    print(\"Power of the initialized transmitter precoder:\", initial_power)\n",
    "\n",
    "  nr_of_iteration_counter = 0 # to keep track of the number of iteration of the WMMSE\n",
    "\n",
    "  while break_condition >= epsilon and nr_of_iteration_counter<=max_nr_of_iterations:\n",
    "    \n",
    "    nr_of_iteration_counter = nr_of_iteration_counter + 1\n",
    "    if log == True:\n",
    "      print(\"WMMSE ITERATION: \", nr_of_iteration_counter)\n",
    "\n",
    "    # Optimize receiver precoder - eq. (5) in the paper of Shi et al.\n",
    "    for user_index_1 in range(nr_of_users):\n",
    "      if user_index_1 in selected_users:\n",
    "        user_interference = 0.0\n",
    "        for user_index_2 in range(nr_of_users):\n",
    "          if user_index_2 in selected_users:\n",
    "            user_interference = user_interference + (np.absolute(np.matmul(np.conj(channel[user_index_1,:]),transmitter_precoder[user_index_2,:])))**2\n",
    "\n",
    "        new_receiver_precoder[user_index_1] = np.matmul(np.conj(channel[user_index_1,:]),transmitter_precoder[user_index_1,:]) / (noise_power + user_interference)\n",
    "\n",
    "    # Optimize mse_weights - eq. (13) in the paper of Shi et al.\n",
    "    for user_index_1 in range(nr_of_users):\n",
    "      if user_index_1 in selected_users:\n",
    "\n",
    "        user_interference = 0 # it includes the channel of all selected users\n",
    "        inter_user_interference = 0 # it includes the channel of all selected users apart from the current one\n",
    "        \n",
    "        for user_index_2 in range(nr_of_users):\n",
    "          if user_index_2 in selected_users:\n",
    "            user_interference = user_interference + (np.absolute(np.matmul(np.conj(channel[user_index_1,:]),transmitter_precoder[user_index_2,:])))**2\n",
    "        for user_index_2 in range(nr_of_users):\n",
    "          if user_index_2 != user_index_1 and user_index_2 in selected_users:\n",
    "            inter_user_interference = inter_user_interference + (np.absolute(np.matmul(np.conj(channel[user_index_1,:]),transmitter_precoder[user_index_2,:])))**2\n",
    "        \n",
    "        new_mse_weights[user_index_1] = (noise_power + user_interference)/(noise_power + inter_user_interference)\n",
    "\n",
    "    A = np.zeros((nr_of_BS_antennas,nr_of_BS_antennas))+1j*np.zeros((nr_of_BS_antennas,nr_of_BS_antennas))\n",
    "    for user_index in range(nr_of_users):\n",
    "      if user_index in selected_users:\n",
    "        # hh should be an hermitian matrix of size (nr_of_BS_antennas X nr_of_BS_antennas)\n",
    "        hh = np.matmul(np.reshape(channel[user_index,:],(nr_of_BS_antennas,1)),np.conj(np.transpose(np.reshape(channel[user_index,:],(nr_of_BS_antennas,1)))))\n",
    "        A = A + (new_mse_weights[user_index]*user_weights[user_index]*(np.absolute(new_receiver_precoder[user_index]))**2)*hh\n",
    "\n",
    "    Sigma_diag_elements_true, U = np.linalg.eigh(A)\n",
    "    Sigma_diag_elements = copy.deepcopy(np.real(Sigma_diag_elements_true))\n",
    "    Lambda = np.zeros((nr_of_BS_antennas,nr_of_BS_antennas)) + 1j*np.zeros((nr_of_BS_antennas,nr_of_BS_antennas))\n",
    "    \n",
    "    for user_index in range(nr_of_users):\n",
    "      if user_index in selected_users:     \n",
    "        hh = np.matmul(np.reshape(channel[user_index,:],(nr_of_BS_antennas,1)),np.conj(np.transpose(np.reshape(channel[user_index,:],(nr_of_BS_antennas,1)))))\n",
    "        Lambda = Lambda + ((user_weights[user_index])**2)*((new_mse_weights[user_index])**2)*((np.absolute(new_receiver_precoder[user_index]))**2)*hh\n",
    "\n",
    "    Phi = np.matmul(np.matmul(np.conj(np.transpose(U)),Lambda),U)\n",
    "    Phi_diag_elements_true = np.diag(Phi)\n",
    "    Phi_diag_elements = copy.deepcopy(Phi_diag_elements_true)\n",
    "    Phi_diag_elements = np.real(Phi_diag_elements)\n",
    "\n",
    "    for i in range(len(Phi_diag_elements)):\n",
    "      if Phi_diag_elements[i]<np.finfo(float).eps:\n",
    "        Phi_diag_elements[i] = np.finfo(float).eps\n",
    "      if (Sigma_diag_elements[i])<np.finfo(float).eps:\n",
    "        Sigma_diag_elements[i] = 0\n",
    "\n",
    "    # Check if mu = 0 is a solution (eq.s (15) and (16) of in the paper of Shi et al.)\n",
    "    power = 0 # the power of transmitter precoder (i.e. sum of the squared norm)\n",
    "    for user_index in range(nr_of_users):\n",
    "      if user_index in selected_users:\n",
    "        if np.linalg.det(A) != 0:\n",
    "          w = np.matmul(np.linalg.inv(A),np.reshape(channel[user_index,:],(nr_of_BS_antennas,1)))*user_weights[user_index]*new_mse_weights[user_index]*(new_receiver_precoder[user_index])\n",
    "          power = power + (compute_norm_of_complex_array(w))**2\n",
    "\n",
    "    # If mu = 0 is a solution, then mu_star = 0\n",
    "    if np.linalg.det(A) != 0 and power <= total_power:\n",
    "      mu_star = 0\n",
    "    # If mu = 0 is not a solution then we search for the \"optimal\" mu by bisection\n",
    "    else:\n",
    "      power_distance = [] # list to store the distance from total_power in the bisection algorithm \n",
    "      mu_low = np.sqrt(1/total_power*np.sum(Phi_diag_elements))\n",
    "      mu_high = 0\n",
    "      low_point = compute_P(Phi_diag_elements, Sigma_diag_elements, mu_low)\n",
    "      high_point = compute_P(Phi_diag_elements, Sigma_diag_elements, mu_high)\n",
    "\n",
    "      obtained_power = total_power + 2*power_tolerance # initialization of the obtained power such that we enter the while \n",
    "\n",
    "      # Bisection search\n",
    "      while np.absolute(total_power - obtained_power) > power_tolerance:\n",
    "        mu_new = (mu_high + mu_low)/2\n",
    "        obtained_power = compute_P(Phi_diag_elements, Sigma_diag_elements, mu_new) # eq. (18) in the paper of Shi et al.\n",
    "        power_distance.append(np.absolute(total_power - obtained_power))\n",
    "        if obtained_power > total_power:\n",
    "          mu_high = mu_new\n",
    "        if obtained_power < total_power:\n",
    "          mu_low = mu_new\n",
    "      mu_star = mu_new\n",
    "      if log == True:\n",
    "        print(\"first value:\", power_distance[0])\n",
    "        plt.title(\"Distance from the target value in bisection (it should decrease)\")\n",
    "        plt.plot(power_distance)\n",
    "        plt.show()\n",
    "\n",
    "    for user_index in range(nr_of_users):\n",
    "      if user_index in selected_users:\n",
    "        new_transmitter_precoder[user_index,:] = np.matmul(np.linalg.inv(A + mu_star*np.eye(nr_of_BS_antennas)),channel[user_index,:])*user_weights[user_index]*new_mse_weights[user_index]*(new_receiver_precoder[user_index]) \n",
    "\n",
    "    # To select only the weights of the selected users to check the break condition\n",
    "    mse_weights_selected_users = []\n",
    "    new_mse_weights_selected_users = []\n",
    "    for user_index in range(nr_of_users): \n",
    "      if user_index in selected_users:\n",
    "        mse_weights_selected_users.append(mse_weights[user_index])\n",
    "        new_mse_weights_selected_users.append(new_mse_weights[user_index])\n",
    "\n",
    "    mse_weights = deepcopy(new_mse_weights)\n",
    "    transmitter_precoder = deepcopy(new_transmitter_precoder)\n",
    "    receiver_precoder = deepcopy(new_receiver_precoder)\n",
    "\n",
    "    WSR.append(compute_weighted_sum_rate(user_weights, channel, transmitter_precoder, noise_power, selected_users))\n",
    "    break_condition = np.absolute(np.log2(np.prod(new_mse_weights_selected_users))-np.log2(np.prod(mse_weights_selected_users)))\n",
    "\n",
    "  if log == True:\n",
    "    plt.title(\"Change of the WSR at each iteration of the WMMSE (it should increase)\")\n",
    "    plt.plot(WSR,'bo')\n",
    "    plt.show()\n",
    "\n",
    "  return transmitter_precoder, receiver_precoder, mse_weights, WSR[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a477e9d8-cbfe-4908-aa44-71a295cbf7d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m user_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((users, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     11\u001b[0m channel \u001b[38;5;241m=\u001b[39m compute_channel(users \u001b[38;5;241m=\u001b[39m users, \n\u001b[1;32m     12\u001b[0m                           antennas \u001b[38;5;241m=\u001b[39m tx_antennas)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mrun_WMMSE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m          \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m          \u001b[49m\u001b[43mselected_users\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduled_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtotal_power\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_power\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m          \u001b[49m\u001b[43mnoise_power\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_weights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muser_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_nr_of_iterations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m, in \u001b[0;36mrun_WMMSE\u001b[0;34m(epsilon, channel, selected_users, total_power, noise_power, user_weights, max_nr_of_iterations, log)\u001b[0m\n\u001b[1;32m     20\u001b[0m transmitter_precoder \u001b[38;5;241m=\u001b[39m transmitter_precoder\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(transmitter_precoder)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msqrt(total_power)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Store the WSR obtained with the initialized trasmitter precoder    \u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m WSR\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcompute_weighted_sum_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransmitter_precoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_power\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_users\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Compute the initial power of the transmitter precoder\u001b[39;00m\n\u001b[1;32m     26\u001b[0m initial_power \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m, in \u001b[0;36mcompute_weighted_sum_rate\u001b[0;34m(user_weights, channel, precoder, noise_power, selected_users)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m user_index \u001b[38;5;129;01min\u001b[39;00m selected_users:\n\u001b[1;32m      7\u001b[0m     user_sinr \u001b[38;5;241m=\u001b[39m compute_sinr(channel, precoder, noise_power, user_index, selected_users)\n\u001b[0;32m----> 8\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m \u001b[43muser_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muser_sinr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "users = 4\n",
    "tx_antennas = 4\n",
    "total_power = 10\n",
    "scheduled_users = [0,1,2,3] \n",
    "\n",
    "#Bad Settings >:(\n",
    "power_tolerance = 0.0001\n",
    "\n",
    "user_weights = np.ones((users, 1))\n",
    "\n",
    "channel = compute_channel(users = users, \n",
    "                          antennas = tx_antennas)\n",
    "\n",
    "run_WMMSE(epsilon = 1e-4, \n",
    "          channel = channel, \n",
    "          selected_users = scheduled_users, \n",
    "          total_power = total_power, \n",
    "          noise_power = 1, \n",
    "          user_weights = user_weights, \n",
    "          max_nr_of_iterations = 100, \n",
    "          log = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d735cb0-cb01-4f8f-b04a-4c10ef52ff1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Works Cited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb22c9-798a-4258-8718-5592bd0839aa",
   "metadata": {},
   "source": [
    "\n",
    "    [1] D. Sun, Y. Xi, A. Yaqot, H. Hellbrück, and H. Wu, “Throughput maximization using deep complex networks for industrial internet of things,” Sensors, vol. 23, no. 2, p. 951, Jan. 2023, doi:10.3390/s23020951.\n",
    "\n",
    "    [2] O. Elijah, S. K. Abdul Rahim, W. K. New, C. Y. Leow, K. Cumanan, and T. Kim Geok, “Intelligent massive MIMO systems for beyond 5G networks: An overview and future trends,” IEEE Access, vol. 10, pp. 102532–102563, 2022, doi: 10.1109/ACCESS.2022.3208284.\n",
    "\n",
    "    [3] L. Pellaco and J. Jaldén, “A matrix-inverse-free implementation of the MU-MIMO WMMSE beamforming algorithm,” IEEE Transactions on Signal Processing, vol. 70, pp. 6360–6375, 2022, doi:10.1109/TSP.2023.3238275.\n",
    "\n",
    "    [4] H. A. Kassir, Z. D. Zaharis, P. I. Lazaridis, N. V. Kantartzis, T. V. Yioultsis, and T. D. Xenos, “A review of the state of the art and future challenges of deep learning-based beamforming,” IEEE Access, vol. 10, pp. 80869–80882, 2022, doi: 10.1109/ACCESS.2022.3195299.\n",
    "\n",
    "    [5] J. Xia and D. Gunduz, “Meta-learning based beamforming design for MISO downlink,” in 2021 IEEE international symposium on information theory (ISIT), IEEE, Jul. 2021. doi:10.1109/isit45174.2021.9518251.\n",
    "\n",
    "    [6] S. Lu, S. Zhao, and Q. Shi, “Learning-based massive beamforming.” 2020. Available: https://arxiv.org/abs/2009.09406\n",
    "\n",
    "    [7] J. Hoydis, S. ten Brink, and M. Debbah, “Massive MIMO in the UL/DL of cellular networks: How many antennas do we need?” IEEE Journal on Selected Areas in Communications, vol. 31, no. 2, pp. 160–171, 2013, doi: 10.1109/JSAC.2013.130205.\n",
    "\n",
    "    [8] J. Zhou and Y. Zhu, “The linear minimum mean-square error estimation with constraints and its applications,” in 2006 international conference on computational intelligence and security, 2006, pp. 1801–1804. doi: 10.1109/ICCIAS.2006.295373.\n",
    "\n",
    "    [9] Q. Shi, M. Razaviyayn, Z.-Q. Luo, and C. He, “An iteratively weighted MMSE approach to distributed sum-utility maximization for a MIMO interfering broadcast channel,” IEEE Transactions on Signal Processing, vol. 59, no. 9, pp. 4331–4340, 2011, doi: 10.1109/TSP.2011.2147784.\n",
    "\n",
    "    [10] M. Akrout, A. Feriani, F. Bellili, A. Mezghani, and E. Hossain, “Domain generalization in machine learning models for wireless communications: Concepts, state-of-the-art, and open issues,” IEEE Communications Surveys & Tutorials, pp. 1–1, 2023, doi: 10.1109/COMST.2023.3326399.\n",
    "\n",
    "    [11] T. Maksymyuk, J. Gazda, O. Yaremko, and D. Nevinskiy, “Deep learning based massive MIMO beamforming for 5G mobile network,” in 2018 IEEE 4th international symposium on wireless systems within the international conferences on intelligent data acquisition and advanced computing systems (IDAACS-SWS), 2018, pp. 241–244. doi: 10.1109/IDAACS-SWS.2018.8525802.\n",
    "\n",
    "    [12] R. Lovato and X. Gong, “Phased antenna array beamforming using convolutional neural networks,”in 2019 IEEE international symposium on antennas and propagation and USNC-URSI radio science meeting, 2019, pp. 1247–1248. doi: 10.1109/APUSNCURSINRSM.2019.8888573. -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
